{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from transformers import BertModel\n",
    "from torchvision import datasets, transforms\n",
    "import mlflow\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from gslTranslater.constants import *\n",
    "from gslTranslater.utils.common import read_yaml, create_directories, save_json\n",
    "from gslTranslater.components.sign_language_translator import SignLanguageTranslator\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    path_of_model: Path\n",
    "    testing_data_dir: Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str\n",
    "    params_image_size: list\n",
    "    params_batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath=CONFIG_FILE_PATH, \n",
    "        params_filepath=PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        eval_config = EvaluationConfig(\n",
    "            path_of_model=self.config.training.trained_model_path,\n",
    "            testing_data_dir=self.config.training.testing_data_dir,\n",
    "            mlflow_uri=self.config.evaluation.mlflow_uri,\n",
    "            all_params=self.params,\n",
    "            params_image_size=self.params.IMAGE_SIZE,\n",
    "            params_batch_size=self.params.BATCH_SIZE\n",
    "        )\n",
    "        return eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def load_model(self) -> torch.nn.Module:\n",
    "        model = SignLanguageTranslator(\n",
    "            cnn_model=models.resnet50(pretrained=False),\n",
    "            transformer_model=BertModel.from_pretrained('nlpaueb/bert-base-greek-uncased-v1'),\n",
    "            tokenizer_len=None\n",
    "        )\n",
    "        model.load_state_dict(torch.load(self.config.path_of_model))\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def _test_loader(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(self.config.params_image_size[:-1]),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        test_dataset = datasets.ImageFolder(root=self.config.testing_data_dir, transform=transform)\n",
    "        return DataLoader(test_dataset, batch_size=self.config.params_batch_size, shuffle=False)\n",
    "\n",
    "    def evaluation(self):\n",
    "        self.model = self.load_model()\n",
    "        test_loader = self._test_loader()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_bleu_scores = []\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.tolist())\n",
    "                all_labels.extend(labels.tolist())\n",
    "\n",
    "                # Calculate BLEU scores for the predictions\n",
    "                for i, pred in enumerate(preds):\n",
    "                    reference = [test_loader.dataset.classes[labels[i]]]\n",
    "                    candidate = [test_loader.dataset.classes[pred]]\n",
    "                    all_bleu_scores.append(sentence_bleu([reference], candidate))\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "        bleu_score_avg = sum(all_bleu_scores) / len(all_bleu_scores)\n",
    "\n",
    "        self.scores = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"bleu_score_avg\": bleu_score_avg\n",
    "        }\n",
    "        self.save_score()\n",
    "\n",
    "    def save_score(self):\n",
    "        save_json(path=Path(\"scores.json\"), data=self.scores)\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics(self.scores)\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.pytorch.log_model(self.model, \"model\", registered_model_name=\"SignLanguageTranslatorModel\")\n",
    "            else:\n",
    "                mlflow.pytorch.log_model(self.model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_evaluation_config()\n",
    "    evaluation = Evaluation(eval_config)\n",
    "    evaluation.evaluation()\n",
    "    evaluation.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
