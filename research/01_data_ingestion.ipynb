{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import mode\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from gslTranslater.constants import *\n",
    "from gslTranslater.utils.common import read_yaml, create_directories, get_size, save_json\n",
    "from gslTranslater import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Dev\\\\Upwork\\\\GSL\\\\GSL-Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_URL: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path\n",
    "    data_dir: Path\n",
    "    analysis_dir: Path\n",
    "    merged_csv: Path\n",
    "    confirmed_csv: Path\n",
    "    missing_csv: Path\n",
    "    gloss_distribution_csv: Path\n",
    "    balanced_csv: Path\n",
    "    summary_csv: Path\n",
    "    frame_count_csv: Path\n",
    "    analysis_txt: Path\n",
    "    train_csv: Path\n",
    "    test_csv: Path\n",
    "    validate_csv: Path\n",
    "    plot_dir: Path\n",
    "    gloss_distribution_plot: Path\n",
    "    max_instances_per_class: int\n",
    "    train_split: float\n",
    "    test_split: float\n",
    "    validate_split: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath=CONFIG_FILE_PATH, \n",
    "        params_filepath=PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "        csv_paths = config.csv_paths\n",
    "        plot_paths = config.plot_paths\n",
    "        \n",
    "        # Create necessary directories\n",
    "        create_directories([config.root_dir, csv_paths.analysis_dir, plot_paths.root_dir])\n",
    "        \n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.tar_dir,\n",
    "            unzip_dir=config.unzip_dir,\n",
    "            data_dir=Path(config.data_dir),\n",
    "            analysis_dir=Path(csv_paths.analysis_dir),\n",
    "            merged_csv=Path(csv_paths.merged_csv),\n",
    "            confirmed_csv=Path(csv_paths.confirmed_csv),\n",
    "            missing_csv=Path(csv_paths.missing_csv),\n",
    "            gloss_distribution_csv=Path(csv_paths.gloss_distribution_csv),\n",
    "            balanced_csv=Path(csv_paths.balanced_csv),\n",
    "            summary_csv=Path(csv_paths.summary_csv),\n",
    "            frame_count_csv=Path(csv_paths.frame_count_csv),\n",
    "            analysis_txt=Path(csv_paths.analysis_txt),\n",
    "            train_csv=Path(config.train_csv),\n",
    "            test_csv=Path(config.test_csv),\n",
    "            validate_csv=Path(config.validate_csv),\n",
    "            plot_dir=Path(plot_paths.root_dir),\n",
    "            gloss_distribution_plot=Path(plot_paths.gloss_distribution_plot),\n",
    "            max_instances_per_class=self.params.MAX_INSTANCES_PER_CLASS,\n",
    "            train_split=self.params.TRAIN_SPLIT,\n",
    "            test_split=self.params.TEST_SPLIT,\n",
    "            validate_split=self.params.VALIDATE_SPLIT\n",
    "        )\n",
    "        \n",
    "        return data_ingestion_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "        # Ensure the analysis and plot directories exist\n",
    "        os.makedirs(self.config.analysis_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.plot_dir, exist_ok=True)\n",
    "\n",
    "    def download_file(self):\n",
    "        if os.path.exists(self.config.unzip_dir):\n",
    "            logger.info(f\"Data already exists at {self.config.unzip_dir}, skipping download and extraction.\")\n",
    "            return\n",
    "        try:\n",
    "            dataset_url = self.config.source_URL\n",
    "            tar_download_dir = self.config.local_data_file\n",
    "            os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "            logger.info(f\"Downloading data from {dataset_url} into file {tar_download_dir}\")\n",
    "\n",
    "            file_id = dataset_url.split(\"/\")[-2]\n",
    "            prefix = 'https://drive.google.com/uc?/export=download&id='\n",
    "            gdown.download(prefix + file_id, tar_download_dir, quiet=False)\n",
    "\n",
    "            logger.info(f\"Downloaded data from {dataset_url} into file {tar_download_dir}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def extract_tar_file(self):\n",
    "        if os.path.exists(self.config.unzip_dir):\n",
    "            logger.info(f\"Data already extracted to {self.config.unzip_dir}, skipping extraction.\")\n",
    "            return\n",
    "        extract_path = self.config.unzip_dir\n",
    "        os.makedirs(extract_path, exist_ok=True)\n",
    "        with tarfile.open(self.config.local_data_file, 'r:gz') as tar_ref:\n",
    "            tar_ref.extractall(extract_path)\n",
    "\n",
    "    def merge_and_clean_csv_files(self):\n",
    "        # List all extracted_annotation CSV files\n",
    "        csv_files = [f for f in os.listdir(self.config.data_dir) if f.startswith('extracted_annotations_')]\n",
    "        merged_df = pd.DataFrame()\n",
    "\n",
    "        def clean_text(text):\n",
    "            if isinstance(text, str):\n",
    "                # Replace pipe symbols with commas\n",
    "                text = text.replace('|', ',')\n",
    "                # Remove any content within parentheses, including the parentheses\n",
    "                text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "                # Remove semicolons\n",
    "                text = text.replace(';', '')\n",
    "            return text\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(os.path.join(self.config.data_dir, csv_file), header=None, names=['Raw'])\n",
    "\n",
    "            # Clean the 'Raw' text\n",
    "            df['Cleaned'] = df['Raw'].apply(clean_text)\n",
    "\n",
    "            # Split the cleaned text into two columns: Path and Gloss\n",
    "            split_df = df['Cleaned'].str.split(',', expand=True)\n",
    "            \n",
    "            if split_df.shape[1] == 2:\n",
    "                split_df.columns = ['Path', 'Gloss']\n",
    "            else:\n",
    "                logger.error(f\"Unexpected format in file {csv_file}. Skipping this file.\")\n",
    "                continue\n",
    "\n",
    "            # Remove any leading/trailing whitespace in 'Path' and 'Gloss'\n",
    "            split_df['Path'] = split_df['Path'].str.strip()\n",
    "            split_df['Gloss'] = split_df['Gloss'].str.strip()\n",
    "\n",
    "            # Concatenate the split_df into the merged_df\n",
    "            merged_df = pd.concat([merged_df, split_df], ignore_index=True)\n",
    "\n",
    "        if merged_df.empty:\n",
    "            logger.error(\"Merged DataFrame is empty. No valid data was found in the CSV files.\")\n",
    "            raise ValueError(\"Merged DataFrame is empty. Ensure that the extracted_annotation CSV files contain valid data.\")\n",
    "\n",
    "        # Save the merged and cleaned CSV with appropriate headers\n",
    "        merged_df.to_csv(self.config.merged_csv, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Merged and cleaned annotations saved to {self.config.merged_csv}\")\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    def check_image_paths(self, merged_df):\n",
    "        confirmed_rows, missing_rows = [], []\n",
    "\n",
    "        for _, row in tqdm(merged_df.iterrows(), total=len(merged_df)):\n",
    "            path = row['Path'].strip()\n",
    "            image_dir = os.path.join(self.config.data_dir, path)\n",
    "\n",
    "            if os.path.exists(image_dir):\n",
    "                images = [img for img in os.listdir(image_dir) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                if len(images) > 0:\n",
    "                    confirmed_rows.append(row)\n",
    "                else:\n",
    "                    missing_rows.append(row)\n",
    "            else:\n",
    "                missing_rows.append(row)\n",
    "\n",
    "        confirmed_df = pd.DataFrame(confirmed_rows, columns=merged_df.columns)\n",
    "        missing_df = pd.DataFrame(missing_rows, columns=merged_df.columns)\n",
    "\n",
    "        # Save the confirmed and missing CSVs\n",
    "        confirmed_df.to_csv(self.config.confirmed_csv, index=False)\n",
    "        missing_df.to_csv(self.config.missing_csv, index=False)\n",
    "        logger.info(f\"Confirmed annotations saved to {self.config.confirmed_csv}\")\n",
    "        logger.info(f\"Missing annotations saved to {self.config.missing_csv}\")\n",
    "\n",
    "        return confirmed_df\n",
    "\n",
    "    def analyze_raw_gloss_distribution(self, merged_df):\n",
    "        gloss_counts = Counter(merged_df['Gloss'])\n",
    "        gloss_distribution_df = pd.DataFrame(list(gloss_counts.items()), columns=['Gloss', 'Count'])\n",
    "        raw_gloss_distribution_plot = self.config.plot_dir / 'raw_gloss_distribution_plot.png'\n",
    "\n",
    "        # Plotting the gloss distribution\n",
    "        sorted_df = gloss_distribution_df.sort_values(by='Count', ascending=False)\n",
    "        plt.figure(figsize=(12, 16))\n",
    "        plt.barh(sorted_df['Gloss'], sorted_df['Count'], color='skyblue')\n",
    "        plt.gca().invert_yaxis()\n",
    "        for index, value in enumerate(sorted_df['Count']):\n",
    "            plt.text(value, index, f'{sorted_df[\"Gloss\"].iloc[index]} ({value})', va='center')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Glosses')\n",
    "        plt.title('Raw Gloss Count Distribution (Highest to Lowest)')\n",
    "\n",
    "        # Save the plot instead of displaying it\n",
    "        plt.savefig(raw_gloss_distribution_plot, bbox_inches='tight')\n",
    "        logger.info(f\"Raw gloss distribution plot saved to {raw_gloss_distribution_plot}\")\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_frames(self, confirmed_df):\n",
    "        confirmed_df['Frame_Count'] = confirmed_df['Path'].apply(self.count_frames)\n",
    "        confirmed_df.to_csv(self.config.frame_count_csv, index=False, encoding='utf-8')\n",
    "        logger.info(f\"CSV with frame counts saved to {self.config.frame_count_csv}\")\n",
    "        \n",
    "        analysis_data = self.calculate_analysis(confirmed_df)\n",
    "        with open(self.config.analysis_txt, 'w', encoding='utf-8') as f:\n",
    "            f.write(analysis_data)\n",
    "        logger.info(f\"Dataset analysis saved to {self.config.analysis_txt}\")\n",
    "\n",
    "    \n",
    "    def count_frames(self, path):\n",
    "        image_dir = os.path.join(self.config.data_dir, path)\n",
    "        frames = [img for img in os.listdir(image_dir) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        return len(frames)\n",
    "    \n",
    "    def calculate_analysis(self, df):\n",
    "        frame_counts = df['Frame_Count']\n",
    "        glosses = df['Gloss']\n",
    "\n",
    "        # Calculate average image dimensions\n",
    "        dimensions = df['Path'].apply(self.get_image_dimensions)\n",
    "        avg_width = np.mean([d[0] for d in dimensions])\n",
    "        avg_height = np.mean([d[1] for d in dimensions])\n",
    "        \n",
    "        # Calculate mode safely\n",
    "        mode_result = mode(frame_counts)\n",
    "        mode_value = mode_result.mode[0] if mode_result.count[0] > 1 else 'No mode'\n",
    "\n",
    "        analysis_data = {\n",
    "            'Max Frames': np.max(frame_counts),\n",
    "            'Min Frames': np.min(frame_counts),\n",
    "            'Average Frames': np.mean(frame_counts),\n",
    "            'Median Frames': np.median(frame_counts),\n",
    "            'Mode Frames': mode_value,\n",
    "            'Gloss with Max Frames': glosses[frame_counts.idxmax()],\n",
    "            'Gloss with Min Frames': glosses[frame_counts.idxmin()],\n",
    "            'Unique Glosses': df['Gloss'].nunique(),\n",
    "            'Total Instances': len(df),\n",
    "            'Average Image Width': avg_width,\n",
    "            'Average Image Height': avg_height\n",
    "        }\n",
    "        \n",
    "        analysis_txt = (\n",
    "            f\"Max Frame Count: {analysis_data['Max Frames']}\\n\"\n",
    "            f\"Gloss with Max Frame Count: {analysis_data['Gloss with Max Frames']}\\n\"\n",
    "            f\"Min Frame Count: {analysis_data['Min Frames']}\\n\"\n",
    "            f\"Gloss with Min Frame Count: {analysis_data['Gloss with Min Frames']}\\n\"\n",
    "            f\"Average Frame Count: {analysis_data['Average Frames']:.2f}\\n\"\n",
    "            f\"Median Frame Count: {analysis_data['Median Frames']}\\n\"\n",
    "            f\"Mode Frame Count: {analysis_data['Mode Frames']}\\n\"\n",
    "            f\"Number of Unique Glosses: {analysis_data['Unique Glosses']}\\n\"\n",
    "            f\"Total Number of Instances: {analysis_data['Total Instances']}\\n\"\n",
    "            f\"Average Image Width: {analysis_data['Average Image Width']:.2f}\\n\"\n",
    "            f\"Average Image Height: {analysis_data['Average Image Height']:.2f}\\n\"\n",
    "        )\n",
    "        \n",
    "        return analysis_txt\n",
    "\n",
    "    def get_image_dimensions(self, path):\n",
    "        image_dir = os.path.join(self.config.data_dir, path)\n",
    "        first_image = os.listdir(image_dir)[0]\n",
    "        image_path = os.path.join(image_dir, first_image)\n",
    "        image = Image.open(image_path)\n",
    "        return image.size  # (width, height)\n",
    "\n",
    "    def analyze_gloss_distribution(self, confirmed_df):\n",
    "        gloss_counts = Counter(confirmed_df['Gloss'])\n",
    "        gloss_distribution_df = pd.DataFrame(list(gloss_counts.items()), columns=['Gloss', 'Count'])\n",
    "        gloss_distribution_df.to_csv(self.config.gloss_distribution_csv, index=False)\n",
    "        logger.info(f\"Gloss distribution saved to {self.config.gloss_distribution_csv}\")\n",
    "\n",
    "        # Plotting the gloss distribution\n",
    "        sorted_df = gloss_distribution_df.sort_values(by='Count', ascending=False)\n",
    "        plt.figure(figsize=(12, 16))\n",
    "        plt.barh(sorted_df['Gloss'], sorted_df['Count'], color='skyblue')\n",
    "        plt.gca().invert_yaxis()\n",
    "        for index, value in enumerate(sorted_df['Count']):\n",
    "            plt.text(value, index, f'{sorted_df[\"Gloss\"].iloc[index]} ({value})', va='center')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Glosses')\n",
    "        plt.title('Gloss Count Distribution (Highest to Lowest)')\n",
    "\n",
    "        # Save the plot instead of displaying it\n",
    "        plt.savefig(self.config.gloss_distribution_plot, bbox_inches='tight')\n",
    "        logger.info(f\"Gloss distribution plot saved to {self.config.gloss_distribution_plot}\")\n",
    "        plt.close()\n",
    "\n",
    "    def create_balanced_dataset(self, confirmed_df):\n",
    "        balanced_df = pd.DataFrame()\n",
    "        summary_data = []\n",
    "\n",
    "        for gloss, group in confirmed_df.groupby('Gloss'):\n",
    "            count = len(group)\n",
    "            if count > self.config.max_instances_per_class:\n",
    "                # Trim the group to the max_instances_per_class\n",
    "                selected_group = group.sample(n=self.config.max_instances_per_class, random_state=42)\n",
    "            else:\n",
    "                selected_group = group\n",
    "            \n",
    "            balanced_df = pd.concat([balanced_df, selected_group], ignore_index=True)\n",
    "            summary_data.append({\"Gloss\": gloss, \"Count\": len(selected_group)})\n",
    "\n",
    "        if balanced_df.empty:\n",
    "            logger.error(\"Balanced DataFrame is empty after trimming. Please check the parameters and the dataset.\")\n",
    "            raise ValueError(\"Balanced DataFrame is empty after trimming. Please check the parameters and the dataset.\")\n",
    "\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        balanced_df.to_csv(self.config.balanced_csv, index=False, encoding='utf-8')\n",
    "        summary_df.to_csv(self.config.summary_csv, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Balanced dataset saved to {self.config.balanced_csv}\")\n",
    "\n",
    "        return balanced_df\n",
    "\n",
    "\n",
    "    def split_data(self, balanced_df):\n",
    "        if balanced_df.empty:\n",
    "            logger.error(\"Balanced DataFrame is empty. Cannot proceed with data splitting.\")\n",
    "            raise ValueError(\"Balanced DataFrame is empty. Ensure the dataset is properly balanced before splitting.\")\n",
    "\n",
    "        total_instances = self.config.max_instances_per_class\n",
    "        train_instances = int(self.config.train_split * total_instances)\n",
    "        test_instances = int(self.config.test_split * total_instances)\n",
    "        validate_instances = total_instances - train_instances - test_instances\n",
    "\n",
    "        train_rows, test_rows, val_rows = [], [], []\n",
    "\n",
    "        for i in range(0, len(balanced_df), total_instances):\n",
    "            group = balanced_df.iloc[i:i+total_instances]\n",
    "            if len(group) == total_instances:\n",
    "                train_rows.append(group.iloc[:train_instances])\n",
    "                test_rows.append(group.iloc[train_instances:train_instances+test_instances])\n",
    "                val_rows.append(group.iloc[train_instances+test_instances:train_instances+test_instances+validate_instances])\n",
    "\n",
    "        if not train_rows or not test_rows or not val_rows:\n",
    "            logger.error(\"No valid groups found for splitting. Check the balance and size of the dataset.\")\n",
    "            raise ValueError(\"No valid groups found for splitting. Check the balance and size of the dataset.\")\n",
    "\n",
    "        train_df = pd.concat(train_rows, ignore_index=True)\n",
    "        test_df = pd.concat(test_rows, ignore_index=True)\n",
    "        val_df = pd.concat(val_rows, ignore_index=True)\n",
    "\n",
    "        train_df.to_csv(self.config.train_csv, index=False)\n",
    "        test_df.to_csv(self.config.test_csv, index=False)\n",
    "        val_df.to_csv(self.config.validate_csv, index=False)\n",
    "\n",
    "        logger.info(f\"Training set saved to {self.config.train_csv}\")\n",
    "        logger.info(f\"Testing set saved to {self.config.test_csv}\")\n",
    "        logger.info(f\"Validation set saved to {self.config.validate_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-30 13:46:36,132: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-08-30 13:46:36,136: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-08-30 13:46:36,137: INFO: common: created directory at: artifacts]\n",
      "[2024-08-30 13:46:36,139: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2024-08-30 13:46:36,140: INFO: common: created directory at: artifacts/data_ingestion/GSL_Analysis]\n",
      "[2024-08-30 13:46:36,142: INFO: common: created directory at: artifacts/data_ingestion/GSL_Analysis/plot_images]\n",
      "[2024-08-30 13:46:36,145: INFO: 246077099: Data already exists at artifacts/data_ingestion/GSL_isolated, skipping download and extraction.]\n",
      "[2024-08-30 13:46:36,145: INFO: 246077099: Data already extracted to artifacts/data_ingestion/GSL_isolated, skipping extraction.]\n",
      "[2024-08-30 13:46:36,159: INFO: 246077099: Merged and cleaned annotations saved to artifacts\\data_ingestion\\GSL_Analysis\\merged_annotations.csv]\n",
      "[2024-08-30 13:46:36,622: INFO: 246077099: Raw gloss distribution plot saved to artifacts\\data_ingestion\\GSL_Analysis\\plot_images\\raw_gloss_distribution_plot.png]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [00:00<00:00, 4550.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-30 13:46:36,676: INFO: 246077099: Confirmed annotations saved to artifacts\\data_ingestion\\GSL_Analysis\\confirmed_annotations.csv]\n",
      "[2024-08-30 13:46:36,676: INFO: 246077099: Missing annotations saved to artifacts\\data_ingestion\\GSL_Analysis\\missing_annotations.csv]\n",
      "[2024-08-30 13:46:36,708: INFO: 246077099: CSV with frame counts saved to artifacts\\data_ingestion\\GSL_Analysis\\merged_with_frame_count.csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-30 13:46:36,824: INFO: 246077099: Dataset analysis saved to artifacts\\data_ingestion\\GSL_Analysis\\dataset_analysis.txt]\n",
      "[2024-08-30 13:46:36,859: INFO: 246077099: Balanced dataset saved to artifacts\\data_ingestion\\GSL_Analysis\\balanced_annotations.csv]\n",
      "[2024-08-30 13:46:36,859: INFO: 246077099: Gloss distribution saved to artifacts\\data_ingestion\\GSL_Analysis\\gloss_distribution.csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALIENWARE M15\\AppData\\Local\\Temp\\ipykernel_18456\\246077099.py:158: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode_result = mode(frame_counts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-30 13:46:37,290: INFO: 246077099: Gloss distribution plot saved to artifacts\\data_ingestion\\GSL_Analysis\\plot_images\\gloss_distribution_plot.png]\n",
      "[2024-08-30 13:46:37,308: INFO: 246077099: Training set saved to artifacts\\data_ingestion\\GSL_isolated\\Greek_isolated\\GSL_isol\\train_annotations.csv]\n",
      "[2024-08-30 13:46:37,309: INFO: 246077099: Testing set saved to artifacts\\data_ingestion\\GSL_isolated\\Greek_isolated\\GSL_isol\\test_annotations.csv]\n",
      "[2024-08-30 13:46:37,310: INFO: 246077099: Validation set saved to artifacts\\data_ingestion\\GSL_isolated\\Greek_isolated\\GSL_isol\\validate_annotations.csv]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "\n",
    "    data_ingestion.download_file()\n",
    "    data_ingestion.extract_tar_file()\n",
    "    merged_df = data_ingestion.merge_and_clean_csv_files()\n",
    "\n",
    "    # Analyze raw gloss distribution\n",
    "    data_ingestion.analyze_raw_gloss_distribution(merged_df)\n",
    "    \n",
    "    confirmed_df = data_ingestion.check_image_paths(merged_df)\n",
    "\n",
    "    # Analyze and process frames\n",
    "    data_ingestion.analyze_frames(confirmed_df)\n",
    "    \n",
    "    # Create the balanced dataset based on max_instances_per_class\n",
    "    balanced_df = data_ingestion.create_balanced_dataset(confirmed_df)\n",
    "\n",
    "    # Analyze gloss distribution after trimming\n",
    "    data_ingestion.analyze_gloss_distribution(balanced_df)\n",
    "    \n",
    "    # Split the data into train, test, and validate\n",
    "    data_ingestion.split_data(balanced_df)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during data ingestion: {str(e)}\")\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
