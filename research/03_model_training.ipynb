{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gslTranslater.constants import *\n",
    "from gslTranslater.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    cnn_lstm_model_path: Path\n",
    "    train_csv: Path\n",
    "    validate_csv: Path\n",
    "    test_csv: Path\n",
    "    data_dir: Path  # Path to the directory where image data is stored\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_image_size: list\n",
    "    max_seq_length: int\n",
    "    learning_rate: float\n",
    "    classes: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath=CONFIG_FILE_PATH, \n",
    "        params_filepath=PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        data_ingestion = self.config.data_ingestion\n",
    "        params = self.params\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            cnn_lstm_model_path=Path(prepare_base_model.cnn_lstm_model_path),\n",
    "            train_csv=Path(data_ingestion.train_csv),\n",
    "            validate_csv=Path(data_ingestion.validate_csv),\n",
    "            test_csv=Path(data_ingestion.test_csv),\n",
    "            data_dir=Path(data_ingestion.data_dir),  # Set the data directory\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            max_seq_length=params.MAX_SEQ_LENGTH,\n",
    "            learning_rate=params.LEARNING_RATE,\n",
    "            classes=params.CLASSES\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "\n",
    "    def load_or_process_dataset(self, csv_path, dataset_type, max_seq_length):\n",
    "        # Define paths for saving the features and labels\n",
    "        features_save_dir = os.path.join(self.config.root_dir, 'features')\n",
    "        os.makedirs(features_save_dir, exist_ok=True)\n",
    "        features_path = os.path.join(features_save_dir, f\"{dataset_type}_features.npy\")\n",
    "        labels_path = os.path.join(features_save_dir, f\"{dataset_type}_labels.npy\")\n",
    "\n",
    "        # Check if the features and labels already exist\n",
    "        if os.path.exists(features_path) and os.path.exists(labels_path):\n",
    "            print(f\"Loading existing {dataset_type} features and labels...\")\n",
    "            data = np.load(features_path)\n",
    "            labels = np.load(labels_path)\n",
    "            return data, labels\n",
    "\n",
    "        print(f\"Processing {dataset_type} dataset...\")\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {dataset_type}\", unit=\"item\", ncols=100):\n",
    "            frames_path = os.path.join(self.config.data_dir, row['Path'].replace('/', '\\\\'))\n",
    "            frames = sorted([os.path.join(frames_path, img) for img in os.listdir(frames_path) if img.endswith('.jpg')])\n",
    "\n",
    "            sequence = []\n",
    "            for frame in frames:\n",
    "                image = tf.keras.preprocessing.image.load_img(frame, target_size=tuple(self.config.params_image_size[:-1]))\n",
    "                image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "                image = tf.keras.applications.resnet.preprocess_input(image)\n",
    "                sequence.append(image)\n",
    "\n",
    "            data.append(sequence)\n",
    "            labels.append(row['Gloss'])\n",
    "\n",
    "        # Pad sequences to ensure uniform shape\n",
    "        data = tf.keras.preprocessing.sequence.pad_sequences(data, maxlen=max_seq_length, padding='post', dtype='float32')\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Save the features and labels\n",
    "        np.save(features_path, data)\n",
    "        np.save(labels_path, labels)\n",
    "\n",
    "        return data, labels\n",
    "\n",
    "    def encode_labels(self, labels):\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels = label_encoder.fit_transform(labels)\n",
    "        labels = tf.keras.utils.to_categorical(labels, num_classes=self.config.classes)\n",
    "        return labels\n",
    "\n",
    "    def load_model(self):\n",
    "        # Load the CNN-LSTM model prepared in Notebook 2\n",
    "        self.model = tf.keras.models.load_model(self.config.cnn_lstm_model_path)\n",
    "\n",
    "    def train(self):\n",
    "        # Load and process the datasets\n",
    "        train_data, train_labels = self.load_or_process_dataset(self.config.train_csv, 'train', self.config.max_seq_length)\n",
    "        validate_data, validate_labels = self.load_or_process_dataset(self.config.validate_csv, 'validate', self.config.max_seq_length)\n",
    "\n",
    "        # Encode the labels to numeric values\n",
    "        train_labels = self.encode_labels(train_labels)\n",
    "        validate_labels = self.encode_labels(validate_labels)\n",
    "\n",
    "        # Load the model\n",
    "        self.load_model()\n",
    "\n",
    "        # Compile the model with the desired optimizer and loss function\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.config.learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            train_data, train_labels,\n",
    "            validation_data=(validate_data, validate_labels),\n",
    "            epochs=self.config.params_epochs,\n",
    "            batch_size=self.config.params_batch_size\n",
    "        )\n",
    "\n",
    "        # Save the trained model\n",
    "        self.model.save(self.config.trained_model_path)\n",
    "        print(f\"Model saved successfully at {self.config.trained_model_path}\")\n",
    "\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-29 00:07:20,836: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-08-29 00:07:20,850: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-08-29 00:07:20,851: INFO: common: created directory at: artifacts]\n",
      "Loading existing train features and labels...\n",
      "Loading existing validate features and labels...\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "[2024-08-29 00:07:36,862: WARNING: load: No training configuration found in save file, so the model was *not* compiled. Compile it manually.]\n",
      "19/19 [==============================] - 3339s 176s/step - loss: 7.2259 - accuracy: 0.0400 - val_loss: 13.0295 - val_accuracy: 0.0400\n",
      "[2024-08-29 01:03:42,336: WARNING: save: Found untraced functions such as _update_step_xla, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.]\n",
      "INFO:tensorflow:Assets written to: artifacts\\training\\cnn_lstm_model_trained.pth\\assets\n",
      "[2024-08-29 01:03:54,818: INFO: builder_impl: Assets written to: artifacts\\training\\cnn_lstm_model_trained.pth\\assets]\n",
      "Model saved successfully at artifacts\\training\\cnn_lstm_model_trained.pth\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.train()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
